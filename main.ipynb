{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b0ce7c",
   "metadata": {},
   "source": [
    "### Campo elegido para el proyecto: Agricultura\n",
    "\n",
    "Asistente para analizar artÃ­culos de agrÃ­cultura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c758d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¤– Bienvenido â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Sistema Agentic AI Multi-Agente</span>                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Universidad Nacional de Colombia - PLN</span>                                                                          <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span> <span style=\"color: #808000; text-decoration-color: #808000\">Procesamiento y AnÃ¡lisis Inteligente de Documentos</span>                                                              <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m ğŸ¤– Bienvenido \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m \u001b[1;36mSistema Agentic AI Multi-Agente\u001b[0m                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m \u001b[37mUniversidad Nacional de Colombia - PLN\u001b[0m                                                                          \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m \u001b[33mProcesamiento y AnÃ¡lisis Inteligente de Documentos\u001b[0m                                                              \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Inicializando sistema...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;36mInicializando sistema\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Cargando modelo de embeddings: all-MiniLM-L6-v2...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mCargando modelo de embeddings: all-MiniLM-L6-v2\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Modelo de embeddings cargado</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Modelo de embeddings cargado\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Ruta de documentos: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\data</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mRuta de documentos: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - \u001b[0m\n",
       "\u001b[36mGITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\data\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Ruta de Ã­ndice: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\faiss_index</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mRuta de Ã­ndice: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - \u001b[0m\n",
       "\u001b[36mGITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\faiss_index\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ“ Ãndice cargado desde: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\faiss_index</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ“ Ãndice cargado desde: c:\\Users\\juana\\OneDrive\\Documentos\\UNAL - \u001b[0m\n",
       "\u001b[32mGITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\faiss_index\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ“ Sistema inicializado exitosamente</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ“ Sistema inicializado exitosamente\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ‘¤ Ingresa tu consulta (o </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">'salir'</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> para terminar): </span></pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸ‘¤ Ingresa tu consulta \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mo \u001b[0m\u001b[1;36m'salir'\u001b[0m\u001b[1;36m para terminar\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m: \u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“ Query del Usuario â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Dame un resumen</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m ğŸ“ Query del Usuario \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m \u001b[1;37mDame un resumen\u001b[0m                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¯ ClasificaciÃ³n de Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">IntenciÃ³n:</span> resumen                                                                                              â”‚\n",
       "â”‚ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Confianza:</span> 0.8                                                                                                  â”‚\n",
       "â”‚ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Razonamiento:</span> El usuario solicita un resumen sin especificar un documento en particular, pero la solicitud      â”‚\n",
       "â”‚ implica que se necesita condensar informaciÃ³n en un formato mÃ¡s breve                                           â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ¯ ClasificaciÃ³n de Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
       "â”‚ \u001b[1;36mIntenciÃ³n:\u001b[0m resumen                                                                                              â”‚\n",
       "â”‚ \u001b[1;36mConfianza:\u001b[0m 0.8                                                                                                  â”‚\n",
       "â”‚ \u001b[1;36mRazonamiento:\u001b[0m El usuario solicita un resumen sin especificar un documento en particular, pero la solicitud      â”‚\n",
       "â”‚ implica que se necesita condensar informaciÃ³n en un formato mÃ¡s breve                                           â”‚\n",
       "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ”„ Procesando consulta con RAG (intenciÃ³n: resumen)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸ”„ Procesando consulta con RAG \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mintenciÃ³n: resumen\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ğŸ” Buscando documentos relevantes (top-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">10</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mğŸ” Buscando documentos relevantes \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mtop-\u001b[0m\u001b[1;33m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'LocalEmbeddings' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 801\u001b[39m\n\u001b[32m    797\u001b[39m             trace_logger.print_trace()\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 775\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Procesar consulta\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m result = \u001b[43morchestrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# Mostrar respuesta\u001b[39;00m\n\u001b[32m    778\u001b[39m console.print(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 597\u001b[39m, in \u001b[36mOrchestrator.process_query\u001b[39m\u001b[34m(self, query, max_retries)\u001b[39m\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_general_query(query)\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    596\u001b[39m     \u001b[38;5;66;03m# Requiere RAG: bÃºsqueda, resumen o comparaciÃ³n\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 641\u001b[39m, in \u001b[36mOrchestrator._handle_rag_query\u001b[39m\u001b[34m(self, query, intent, max_retries)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;66;03m# Paso 1: Recuperar documentos relevantes\u001b[39;00m\n\u001b[32m    640\u001b[39m k = \u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m intent == \u001b[33m\"\u001b[39m\u001b[33mbusqueda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10\u001b[39m  \u001b[38;5;66;03m# MÃ¡s docs para resumen/comparaciÃ³n\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m relevant_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m relevant_docs:\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    645\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m    646\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mintent\u001b[39m\u001b[33m\"\u001b[39m: intent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.trace_logger.get_trace()\n\u001b[32m    651\u001b[39m     }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 354\u001b[39m, in \u001b[36mSemanticRetriever.retrieve\u001b[39m\u001b[34m(self, query, k)\u001b[39m\n\u001b[32m    351\u001b[39m console.print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[bold yellow]ğŸ” Buscando documentos relevantes (top-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...[/bold yellow]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# BÃºsqueda de similaridad en FAISS\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m relevant_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28mself\u001b[39m.trace_logger.log_event(\n\u001b[32m    357\u001b[39m     agent=\u001b[33m\"\u001b[39m\u001b[33mSemanticRetriever\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    358\u001b[39m     action=\u001b[33m\"\u001b[39m\u001b[33mretrieve_documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     }\n\u001b[32m    364\u001b[39m )\n\u001b[32m    366\u001b[39m console.print(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[green]âœ“ Recuperados \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(relevant_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documentos relevantes[/green]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juana\\OneDrive\\Documentos\\UNAL - GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juana\\OneDrive\\Documentos\\UNAL - GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juana\\OneDrive\\Documentos\\UNAL - GITHUB\\UNAL_2025_2\\PLN\\Sistema-multiagente-para-el-procesamiento-y-an-lisis-de-documentos-\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:268\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function.embed_query(text)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'LocalEmbeddings' object is not callable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sistema Agentic AI Multi-Agente para Procesamiento de Documentos\n",
    "Universidad Nacional de Colombia - PLN\n",
    "IntegraciÃ³n con Groq (modelos Llama) y embeddings locales\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Sentence Transformers para embeddings locales\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import correcto para RecursiveCharacterTextSplitter\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "except ImportError:\n",
    "    try:\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    except ImportError:\n",
    "        from langchain_core.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Importaciones para cargar documentos\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredHTMLLoader\n",
    "\n",
    "# Rich para visualizaciÃ³n\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.tree import Tree\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# ConfiguraciÃ³n de consola\n",
    "console = Console()\n",
    "\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    \"\"\"\n",
    "    Clase para generar embeddings locales usando Sentence Transformers\n",
    "    Compatible con FAISS y LangChain\n",
    "    No requiere API keys y funciona offline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Inicializar modelo de embeddings\n",
    "        Modelos recomendados:\n",
    "        - all-MiniLM-L6-v2: RÃ¡pido, ligero (80MB), buena calidad\n",
    "        - paraphrase-multilingual-MiniLM-L12-v2: Mejor para espaÃ±ol\n",
    "        \"\"\"\n",
    "        console.print(f\"[yellow]Cargando modelo de embeddings: {model_name}...[/yellow]\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        console.print(\"[green]âœ“ Modelo de embeddings cargado[/green]\")\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generar embeddings para una lista de documentos\"\"\"\n",
    "        embeddings = self.model.encode(\n",
    "            texts, \n",
    "            show_progress_bar=True, \n",
    "            convert_to_numpy=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Generar embedding para una consulta\"\"\"\n",
    "        embedding = self.model.encode([text], convert_to_numpy=True)\n",
    "        return embedding[0].tolist()\n",
    "    \n",
    "    def __call__(self, text: str) -> List[float]:\n",
    "        \"\"\"Hacer la clase callable para compatibilidad con FAISS\"\"\"\n",
    "        return self.embed_query(text)\n",
    "\n",
    "\n",
    "class TraceabilityLogger:\n",
    "    \"\"\"Clase para mantener trazabilidad completa del sistema\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trace_log = []\n",
    "        self.start_time = None\n",
    "        \n",
    "    def start_trace(self, query: str):\n",
    "        \"\"\"Iniciar trazabilidad de una consulta\"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.trace_log = [{\n",
    "            \"timestamp\": self.start_time.isoformat(),\n",
    "            \"event\": \"query_received\",\n",
    "            \"query\": query\n",
    "        }]\n",
    "        \n",
    "    def log_event(self, agent: str, action: str, details: Dict[str, Any]):\n",
    "        \"\"\"Registrar evento en el log de trazabilidad\"\"\"\n",
    "        self.trace_log.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"agent\": agent,\n",
    "            \"action\": action,\n",
    "            \"details\": details\n",
    "        })\n",
    "        \n",
    "    def get_trace(self) -> List[Dict]:\n",
    "        \"\"\"Obtener el log completo de trazabilidad\"\"\"\n",
    "        return self.trace_log\n",
    "    \n",
    "    def print_trace(self):\n",
    "        \"\"\"Imprimir trazabilidad de forma visual\"\"\"\n",
    "        tree = Tree(\"ğŸ” [bold blue]Trazabilidad del Sistema[/bold blue]\")\n",
    "        \n",
    "        for entry in self.trace_log:\n",
    "            if entry.get(\"event\") == \"query_received\":\n",
    "                tree.add(f\"[green]Query recibida:[/green] {entry['query']}\")\n",
    "            else:\n",
    "                agent_node = tree.add(f\"[yellow]{entry['agent']}[/yellow]\")\n",
    "                agent_node.add(f\"[cyan]AcciÃ³n:[/cyan] {entry['action']}\")\n",
    "                if entry.get(\"details\"):\n",
    "                    details_str = json.dumps(entry[\"details\"], indent=2, ensure_ascii=False)\n",
    "                    agent_node.add(f\"[magenta]Detalles:[/magenta]\\n{details_str}\")\n",
    "        \n",
    "        console.print(tree)\n",
    "\n",
    "\n",
    "class DocumentIndexer:\n",
    "    \"\"\"\n",
    "    Agente 1: Consumo e IndexaciÃ³n de Documentos\n",
    "    - Carga documentos (PDF/TXT/HTML)\n",
    "    - Limpia, segmenta en chunks y genera embeddings\n",
    "    - Indexa en FAISS\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_model, trace_logger: TraceabilityLogger):\n",
    "        self.embeddings = embeddings_model\n",
    "        self.trace_logger = trace_logger\n",
    "        self.vector_store = None\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_documents(self, directory_path: str) -> List[Document]:\n",
    "        \"\"\"Cargar documentos desde un directorio\"\"\"\n",
    "        console.print(f\"[bold green]ğŸ“‚ Cargando documentos desde: {directory_path}[/bold green]\")\n",
    "        \n",
    "        documents = []\n",
    "        path = Path(directory_path)\n",
    "        \n",
    "        # Extensiones soportadas\n",
    "        loaders_map = {\n",
    "            '.pdf': PyPDFLoader,\n",
    "            '.txt': TextLoader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.htm': UnstructuredHTMLLoader\n",
    "        }\n",
    "        \n",
    "        file_count = 0\n",
    "        for file_path in path.rglob('*'):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in loaders_map:\n",
    "                try:\n",
    "                    loader_class = loaders_map[file_path.suffix.lower()]\n",
    "                    \n",
    "                    if file_path.suffix.lower() == '.txt':\n",
    "                        loader = loader_class(str(file_path), encoding='utf-8')\n",
    "                    else:\n",
    "                        loader = loader_class(str(file_path))\n",
    "                    \n",
    "                    docs = loader.load()\n",
    "                    documents.extend(docs)\n",
    "                    file_count += 1\n",
    "                    \n",
    "                    if file_count % 10 == 0:\n",
    "                        console.print(f\"  âœ“ Cargados {file_count} documentos...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    console.print(f\"[red]  âœ— Error al cargar {file_path.name}: {str(e)}[/red]\")\n",
    "                    continue\n",
    "        \n",
    "        console.print(f\"[bold green]âœ“ Total de documentos cargados: {file_count}[/bold green]\")\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"DocumentIndexer\",\n",
    "            action=\"load_documents\",\n",
    "            details={\"total_documents\": file_count, \"directory\": directory_path}\n",
    "        )\n",
    "        \n",
    "        self.documents = documents\n",
    "        return documents\n",
    "    \n",
    "    def process_and_index(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"Procesar documentos: chunking + embeddings + indexaciÃ³n en FAISS\"\"\"\n",
    "        console.print(\"[bold yellow]ğŸ”¨ Procesando documentos...[/bold yellow]\")\n",
    "        \n",
    "        # Dividir en chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        console.print(f\"  âœ“ Documentos divididos en {len(chunks)} chunks\")\n",
    "        \n",
    "        # Crear vector store con FAISS\n",
    "        console.print(\"[bold yellow]ğŸ§® Generando embeddings e indexando en FAISS...[/bold yellow]\")\n",
    "        self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
    "        console.print(\"[bold green]âœ“ IndexaciÃ³n completada[/bold green]\")\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"DocumentIndexer\",\n",
    "            action=\"process_and_index\",\n",
    "            details={\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"chunk_overlap\": chunk_overlap\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return self.vector_store\n",
    "    \n",
    "    def save_index(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"Guardar Ã­ndice FAISS en disco\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "            console.print(f\"[green]âœ“ Ãndice guardado en: {path}[/green]\")\n",
    "    \n",
    "    def load_index(self, path: str = \"faiss_index\"):\n",
    "        \"\"\"Cargar Ã­ndice FAISS desde disco\"\"\"\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                path, \n",
    "                self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            console.print(f\"[green]âœ“ Ãndice cargado desde: {path}[/green]\")\n",
    "            return self.vector_store\n",
    "        except Exception as e:\n",
    "            console.print(f\"[red]âœ— Error al cargar Ã­ndice: {str(e)}[/red]\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    \"\"\"\n",
    "    Agente 2: Clasificador de Consultas\n",
    "    Clasifica la intenciÃ³n del usuario en 4 categorÃ­as:\n",
    "    - bÃºsqueda: Consulta especÃ­fica en documentos\n",
    "    - resumen: Resumen de documentos\n",
    "    - comparaciÃ³n: Comparar documentos\n",
    "    - general: Pregunta general sin necesidad de RAG\n",
    "    \n",
    "    Usa Groq Llama 3.3 70B para interpretaciÃ³n profunda del lenguaje\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceabilityLogger):\n",
    "        self.trace_logger = trace_logger\n",
    "        # Groq Llama 3.3 70B para interpretaciÃ³n profunda del lenguaje\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.1,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def classify(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Clasificar la intenciÃ³n de la consulta del usuario\"\"\"\n",
    "        \n",
    "        classification_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Eres un clasificador de consultas experto. Debes clasificar la intenciÃ³n del usuario en una de estas categorÃ­as:\n",
    "\n",
    "1. \"busqueda\": El usuario solicita informaciÃ³n especÃ­fica, hechos o datos contenidos en documentos\n",
    "2. \"resumen\": El usuario quiere un resumen de uno o varios documentos\n",
    "3. \"comparacion\": El usuario quiere comparar o contrastar informaciÃ³n de diferentes documentos\n",
    "4. \"general\": El usuario hace una pregunta general que no requiere buscar en documentos especÃ­ficos\n",
    "\n",
    "Ejemplos:\n",
    "- \"Â¿CuÃ¡l es la capital de Francia?\" -> general\n",
    "- \"Â¿QuÃ© dice el documento sobre las causas de la guerra?\" -> busqueda\n",
    "- \"Resume el contenido del artÃ­culo sobre IA\" -> resumen\n",
    "- \"Compara las conclusiones de estos dos estudios\" -> comparacion\n",
    "- \"ExplÃ­came quÃ© es la fotosÃ­ntesis\" -> general\n",
    "- \"Â¿QuÃ© informaciÃ³n hay sobre cambio climÃ¡tico?\" -> busqueda\n",
    "\n",
    "Responde SOLO con un JSON en este formato exacto:\n",
    "{{\"intencion\": \"busqueda|resumen|comparacion|general\", \"confianza\": 0.95, \"razonamiento\": \"breve explicaciÃ³n\"}}\"\"\"),\n",
    "            (\"user\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        chain = classification_prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\"query\": query})\n",
    "        \n",
    "        # Parsear respuesta\n",
    "        try:\n",
    "            # Limpiar respuesta si tiene markdown\n",
    "            response_clean = response.strip()\n",
    "            if response_clean.startswith(\"```json\"):\n",
    "                response_clean = response_clean[7:]\n",
    "            if response_clean.endswith(\"```\"):\n",
    "                response_clean = response_clean[:-3]\n",
    "            \n",
    "            result = json.loads(response_clean.strip())\n",
    "            \n",
    "            self.trace_logger.log_event(\n",
    "                agent=\"QueryClassifier\",\n",
    "                action=\"classify_intent\",\n",
    "                details={\n",
    "                    \"query\": query,\n",
    "                    \"clasificacion\": result\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            console.print(Panel(\n",
    "                f\"[bold cyan]IntenciÃ³n:[/bold cyan] {result['intencion']}\\n\"\n",
    "                f\"[bold cyan]Confianza:[/bold cyan] {result['confianza']}\\n\"\n",
    "                f\"[bold cyan]Razonamiento:[/bold cyan] {result['razonamiento']}\",\n",
    "                title=\"ğŸ¯ ClasificaciÃ³n de Query\"\n",
    "            ))\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            console.print(\"[red]Error al parsear clasificaciÃ³n, usando 'busqueda' por defecto[/red]\")\n",
    "            return {\n",
    "                \"intencion\": \"busqueda\",\n",
    "                \"confianza\": 0.5,\n",
    "                \"razonamiento\": \"Error en clasificaciÃ³n, asumiendo bÃºsqueda\"\n",
    "            }\n",
    "\n",
    "\n",
    "class SemanticRetriever:\n",
    "    \"\"\"\n",
    "    Agente 3: Recuperador SemÃ¡ntico\n",
    "    - Ejecuta bÃºsqueda de similaridad semÃ¡ntica en FAISS\n",
    "    - Selecciona documentos mÃ¡s relevantes\n",
    "    \n",
    "    Usa Groq Llama 3.1 8B para optimizar velocidad de recuperaciÃ³n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: FAISS, trace_logger: TraceabilityLogger):\n",
    "        self.vector_store = vector_store\n",
    "        self.trace_logger = trace_logger\n",
    "        # Groq Llama 3.1 8B para velocidad de recuperaciÃ³n\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Recuperar documentos relevantes mediante bÃºsqueda semÃ¡ntica\"\"\"\n",
    "        \n",
    "        console.print(f\"[bold yellow]ğŸ” Buscando documentos relevantes (top-{k})...[/bold yellow]\")\n",
    "        \n",
    "        # BÃºsqueda de similaridad en FAISS\n",
    "        relevant_docs = self.vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"SemanticRetriever\",\n",
    "            action=\"retrieve_documents\",\n",
    "            details={\n",
    "                \"query\": query,\n",
    "                \"num_docs_retrieved\": len(relevant_docs),\n",
    "                \"doc_sources\": [doc.metadata.get('source', 'unknown') for doc in relevant_docs]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        console.print(f\"[green]âœ“ Recuperados {len(relevant_docs)} documentos relevantes[/green]\")\n",
    "        \n",
    "        return relevant_docs\n",
    "\n",
    "\n",
    "class RAGResponseGenerator:\n",
    "    \"\"\"\n",
    "    Agente 4: Generador de Respuestas con RAG\n",
    "    - Construye respuesta combinando query + contexto recuperado\n",
    "    - Produce respuestas justificadas con citas\n",
    "    \n",
    "    Usa Groq Llama 3.3 70B para generar respuestas rÃ¡pidas basadas en contexto\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceabilityLogger):\n",
    "        self.trace_logger = trace_logger\n",
    "        # Groq Llama 3.3 70B para respuestas de alta calidad\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.3,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def generate_response(self, query: str, context_docs: List[Document], intent: str) -> str:\n",
    "        \"\"\"Generar respuesta usando RAG\"\"\"\n",
    "        \n",
    "        console.print(\"[bold yellow]âœï¸ Generando respuesta con RAG...[/bold yellow]\")\n",
    "        \n",
    "        # Preparar contexto\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"--- Documento {i+1} (Fuente: {doc.metadata.get('source', 'desconocida')}) ---\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(context_docs)\n",
    "        ])\n",
    "        \n",
    "        # Prompt segÃºn intenciÃ³n\n",
    "        if intent == \"resumen\":\n",
    "            system_prompt = \"\"\"Eres un asistente experto en resumir documentos. \n",
    "            Genera un resumen claro y conciso del contenido proporcionado.\n",
    "            Incluye las ideas principales y cita las fuentes cuando sea apropiado.\"\"\"\n",
    "        elif intent == \"comparacion\":\n",
    "            system_prompt = \"\"\"Eres un asistente experto en anÃ¡lisis comparativo.\n",
    "            Compara y contrasta la informaciÃ³n de los documentos proporcionados.\n",
    "            Identifica similitudes, diferencias y cita las fuentes de cada afirmaciÃ³n.\"\"\"\n",
    "        else:  # bÃºsqueda\n",
    "            system_prompt = \"\"\"Eres un asistente experto en responder preguntas basÃ¡ndote en documentos.\n",
    "            Responde la pregunta del usuario usando ÃšNICAMENTE la informaciÃ³n del contexto proporcionado.\n",
    "            IMPORTANTE: Cita las fuentes de donde obtienes la informaciÃ³n.\n",
    "            Si la informaciÃ³n no estÃ¡ en el contexto, indÃ­calo claramente.\"\"\"\n",
    "        \n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", \"\"\"Contexto de los documentos:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario: {query}\n",
    "\n",
    "Responde de forma clara, precisa y justificada con citas a las fuentes.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = rag_prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\"context\": context, \"query\": query})\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"RAGResponseGenerator\",\n",
    "            action=\"generate_response\",\n",
    "            details={\n",
    "                \"query\": query,\n",
    "                \"intent\": intent,\n",
    "                \"num_context_docs\": len(context_docs),\n",
    "                \"response_length\": len(response)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        console.print(\"[green]âœ“ Respuesta generada[/green]\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "class ResponseVerifier:\n",
    "    \"\"\"\n",
    "    Agente 5: Verificador/CrÃ­tico de Respuestas\n",
    "    - EvalÃºa si la respuesta estÃ¡ respaldada por el contexto\n",
    "    - Verifica coherencia y evita alucinaciones\n",
    "    - Valida cumplimiento de requerimientos\n",
    "    \n",
    "    Usa Groq Llama 3.3 70B para razonamiento y validaciÃ³n compleja\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trace_logger: TraceabilityLogger):\n",
    "        self.trace_logger = trace_logger\n",
    "        # Groq Llama 3.3 70B para razonamiento y validaciÃ³n compleja\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.1,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def verify(self, query: str, response: str, context_docs: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"Verificar calidad y coherencia de la respuesta\"\"\"\n",
    "        \n",
    "        console.print(\"[bold yellow]âœ… Verificando respuesta...[/bold yellow]\")\n",
    "        \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        verification_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Eres un evaluador crÃ­tico de respuestas. Tu trabajo es verificar:\n",
    "\n",
    "1. Â¿La respuesta estÃ¡ respaldada por el contexto proporcionado?\n",
    "2. Â¿La respuesta es coherente y lÃ³gica?\n",
    "3. Â¿Hay signos de alucinaciÃ³n (informaciÃ³n inventada no presente en el contexto)?\n",
    "4. Â¿La respuesta responde adecuadamente a la pregunta del usuario?\n",
    "\n",
    "Responde con un JSON en este formato:\n",
    "{{\n",
    "    \"es_valida\": true/false,\n",
    "    \"confianza\": 0.0-1.0,\n",
    "    \"problemas_detectados\": [\"lista de problemas si existen\"],\n",
    "    \"sugerencias_mejora\": [\"sugerencias si es_valida es false\"],\n",
    "    \"razonamiento\": \"explicaciÃ³n de tu evaluaciÃ³n\"\n",
    "}}\"\"\"),\n",
    "            (\"user\", \"\"\"Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta del usuario: {query}\n",
    "\n",
    "Respuesta a evaluar: {response}\n",
    "\n",
    "EvalÃºa la respuesta.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = verification_prompt | self.llm | StrOutputParser()\n",
    "        verification_result = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"query\": query,\n",
    "            \"response\": response\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            # Limpiar respuesta\n",
    "            verification_clean = verification_result.strip()\n",
    "            if verification_clean.startswith(\"```json\"):\n",
    "                verification_clean = verification_clean[7:]\n",
    "            if verification_clean.endswith(\"```\"):\n",
    "                verification_clean = verification_clean[:-3]\n",
    "            \n",
    "            result = json.loads(verification_clean.strip())\n",
    "            \n",
    "            self.trace_logger.log_event(\n",
    "                agent=\"ResponseVerifier\",\n",
    "                action=\"verify_response\",\n",
    "                details=result\n",
    "            )\n",
    "            \n",
    "            if result[\"es_valida\"]:\n",
    "                console.print(\"[bold green]âœ“ Respuesta verificada exitosamente[/bold green]\")\n",
    "            else:\n",
    "                console.print(\"[bold red]âœ— Respuesta requiere mejoras[/bold red]\")\n",
    "                console.print(f\"[yellow]Problemas: {', '.join(result['problemas_detectados'])}[/yellow]\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            console.print(\"[red]Error al parsear verificaciÃ³n[/red]\")\n",
    "            return {\n",
    "                \"es_valida\": True,\n",
    "                \"confianza\": 0.5,\n",
    "                \"problemas_detectados\": [],\n",
    "                \"sugerencias_mejora\": [],\n",
    "                \"razonamiento\": \"Error en verificaciÃ³n, aceptando respuesta\"\n",
    "            }\n",
    "\n",
    "\n",
    "class Orchestrator:\n",
    "    \"\"\"\n",
    "    Agente Orquestador Principal\n",
    "    - Administra el flujo completo del sistema\n",
    "    - Determina quÃ© agente ejecutar segÃºn la consulta\n",
    "    - Coordina la interacciÃ³n entre agentes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        indexer: DocumentIndexer,\n",
    "        classifier: QueryClassifier,\n",
    "        retriever: SemanticRetriever,\n",
    "        generator: RAGResponseGenerator,\n",
    "        verifier: ResponseVerifier,\n",
    "        trace_logger: TraceabilityLogger\n",
    "    ):\n",
    "        self.indexer = indexer\n",
    "        self.classifier = classifier\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.verifier = verifier\n",
    "        self.trace_logger = trace_logger\n",
    "        \n",
    "        # LLM para decisiones del orquestador (Llama 3.1 8B por velocidad)\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "    def process_query(self, query: str, max_retries: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"Procesar consulta del usuario orquestando todos los agentes\"\"\"\n",
    "        \n",
    "        console.print(Panel(\n",
    "            f\"[bold white]{query}[/bold white]\",\n",
    "            title=\"ğŸ“ Query del Usuario\",\n",
    "            border_style=\"blue\"\n",
    "        ))\n",
    "        \n",
    "        # Iniciar trazabilidad\n",
    "        self.trace_logger.start_trace(query)\n",
    "        \n",
    "        # Paso 1: Clasificar intenciÃ³n\n",
    "        classification = self.classifier.classify(query)\n",
    "        intent = classification[\"intencion\"]\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"Orchestrator\",\n",
    "            action=\"route_query\",\n",
    "            details={\"intent\": intent}\n",
    "        )\n",
    "        \n",
    "        # Paso 2: Rutear segÃºn intenciÃ³n\n",
    "        if intent == \"general\":\n",
    "            # Respuesta directa sin RAG\n",
    "            return self._handle_general_query(query)\n",
    "        else:\n",
    "            # Requiere RAG: bÃºsqueda, resumen o comparaciÃ³n\n",
    "            return self._handle_rag_query(query, intent, max_retries)\n",
    "    \n",
    "    def _handle_general_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Manejar consulta general sin necesidad de RAG\"\"\"\n",
    "        \n",
    "        console.print(\"[bold cyan]ğŸ’¬ Procesando consulta general (sin RAG)...[/bold cyan]\")\n",
    "        \n",
    "        # Usar Groq Llama 3.3 70B para respuestas generales de calidad\n",
    "        llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.7,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        general_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Eres un asistente Ãºtil y conocedor. Responde las preguntas del usuario de manera clara y precisa.\"),\n",
    "            (\"user\", \"{query}\")\n",
    "        ])\n",
    "        \n",
    "        chain = general_prompt | llm | StrOutputParser()\n",
    "        response = chain.invoke({\"query\": query})\n",
    "        \n",
    "        self.trace_logger.log_event(\n",
    "            agent=\"Orchestrator\",\n",
    "            action=\"handle_general_query\",\n",
    "            details={\"response_length\": len(response)}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"intent\": \"general\",\n",
    "            \"response\": response,\n",
    "            \"documents_used\": [],\n",
    "            \"verification\": {\"es_valida\": True, \"confianza\": 1.0},\n",
    "            \"trace\": self.trace_logger.get_trace()\n",
    "        }\n",
    "    \n",
    "    def _handle_rag_query(self, query: str, intent: str, max_retries: int) -> Dict[str, Any]:\n",
    "        \"\"\"Manejar consulta que requiere RAG\"\"\"\n",
    "        \n",
    "        console.print(f\"[bold cyan]ğŸ”„ Procesando consulta con RAG (intenciÃ³n: {intent})...[/bold cyan]\")\n",
    "        \n",
    "        # Paso 1: Recuperar documentos relevantes\n",
    "        k = 5 if intent == \"busqueda\" else 10  # MÃ¡s docs para resumen/comparaciÃ³n\n",
    "        relevant_docs = self.retriever.retrieve(query, k=k)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"intent\": intent,\n",
    "                \"response\": \"No se encontraron documentos relevantes para responder a tu consulta.\",\n",
    "                \"documents_used\": [],\n",
    "                \"verification\": {\"es_valida\": False, \"confianza\": 0.0},\n",
    "                \"trace\": self.trace_logger.get_trace()\n",
    "            }\n",
    "        \n",
    "        # Paso 2: Generar respuesta con RAG (con loop de verificaciÃ³n)\n",
    "        for attempt in range(max_retries + 1):\n",
    "            console.print(f\"[cyan]Intento {attempt + 1}/{max_retries + 1} de generaciÃ³n...[/cyan]\")\n",
    "            \n",
    "            response = self.generator.generate_response(query, relevant_docs, intent)\n",
    "            \n",
    "            # Paso 3: Verificar respuesta\n",
    "            verification = self.verifier.verify(query, response, relevant_docs)\n",
    "            \n",
    "            if verification[\"es_valida\"] or attempt == max_retries:\n",
    "                # Respuesta vÃ¡lida o se alcanzÃ³ mÃ¡ximo de intentos\n",
    "                return {\n",
    "                    \"query\": query,\n",
    "                    \"intent\": intent,\n",
    "                    \"response\": response,\n",
    "                    \"documents_used\": [doc.metadata.get('source', 'unknown') for doc in relevant_docs],\n",
    "                    \"verification\": verification,\n",
    "                    \"attempts\": attempt + 1,\n",
    "                    \"trace\": self.trace_logger.get_trace()\n",
    "                }\n",
    "            \n",
    "            console.print(\"[yellow]âš ï¸ Respuesta no vÃ¡lida, regenerando...[/yellow]\")\n",
    "        \n",
    "        # Esto no deberÃ­a alcanzarse nunca\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"response\": response,\n",
    "            \"documents_used\": [doc.metadata.get('source', 'unknown') for doc in relevant_docs],\n",
    "            \"verification\": verification,\n",
    "            \"trace\": self.trace_logger.get_trace()\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"FunciÃ³n principal del sistema\"\"\"\n",
    "    \n",
    "    console.print(Panel(\n",
    "        \"[bold cyan]Sistema Agentic AI Multi-Agente[/bold cyan]\\n\"\n",
    "        \"[white]Universidad Nacional de Colombia - PLN[/white]\\n\"\n",
    "        \"[yellow]Procesamiento y AnÃ¡lisis Inteligente de Documentos[/yellow]\",\n",
    "        title=\"ğŸ¤– Bienvenido\",\n",
    "        border_style=\"green\"\n",
    "    ))\n",
    "    \n",
    "    # Verificar variable de entorno\n",
    "    if not os.getenv(\"GROQ_API_KEY\"):\n",
    "        console.print(\"[bold red]Error: GROQ_API_KEY no configurada[/bold red]\")\n",
    "        console.print(\"[yellow]Por favor, aÃ±ade tu API key de Groq al archivo .env[/yellow]\")\n",
    "        console.print(\"[yellow]ObtÃ©nla en: https://console.groq.com/keys[/yellow]\")\n",
    "        return\n",
    "    \n",
    "    # Inicializar componentes\n",
    "    console.print(\"\\n[bold cyan]Inicializando sistema...[/bold cyan]\")\n",
    "    \n",
    "    # Embeddings locales (no requiere API)\n",
    "    embeddings = LocalEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    trace_logger = TraceabilityLogger()\n",
    "    \n",
    "    # Inicializar agentes\n",
    "    indexer = DocumentIndexer(embeddings, trace_logger)\n",
    "    \n",
    "    # Obtener rutas desde .env o usar defaults relativos\n",
    "    documents_path = os.getenv(\"DOCUMENTS_PATH\", \"./data\")\n",
    "    index_path = os.getenv(\"FAISS_INDEX_PATH\", \"./faiss_index\")\n",
    "    \n",
    "    # Convertir a ruta absoluta para mejor manejo\n",
    "    documents_path = os.path.abspath(documents_path)\n",
    "    index_path = os.path.abspath(index_path)\n",
    "    \n",
    "    console.print(f\"[cyan]Ruta de documentos: {documents_path}[/cyan]\")\n",
    "    console.print(f\"[cyan]Ruta de Ã­ndice: {index_path}[/cyan]\")\n",
    "    \n",
    "    # Cargar o crear Ã­ndice\n",
    "    vector_store = indexer.load_index(index_path)\n",
    "    \n",
    "    if vector_store is None:\n",
    "        # Verificar que existe el directorio de documentos\n",
    "        if not os.path.exists(documents_path):\n",
    "            console.print(f\"[bold red]Error: No se encuentra el directorio de documentos:[/bold red]\")\n",
    "            console.print(f\"[yellow]{documents_path}[/yellow]\")\n",
    "            console.print(f\"[cyan]Crea la carpeta 'data' en el directorio del proyecto y coloca tus documentos allÃ­[/cyan]\")\n",
    "            return\n",
    "        \n",
    "        # Cargar y procesar documentos\n",
    "        documents = indexer.load_documents(documents_path)\n",
    "        \n",
    "        if not documents:\n",
    "            console.print(\"[bold red]No se encontraron documentos para procesar[/bold red]\")\n",
    "            console.print(\"[yellow]Formatos soportados: PDF, TXT, HTML[/yellow]\")\n",
    "            return\n",
    "        \n",
    "        vector_store = indexer.process_and_index(documents)\n",
    "        indexer.save_index(index_path)\n",
    "    \n",
    "    # Inicializar resto de agentes\n",
    "    classifier = QueryClassifier(trace_logger)\n",
    "    retriever = SemanticRetriever(vector_store, trace_logger)\n",
    "    generator = RAGResponseGenerator(trace_logger)\n",
    "    verifier = ResponseVerifier(trace_logger)\n",
    "    \n",
    "    # Inicializar orquestador\n",
    "    orchestrator = Orchestrator(\n",
    "        indexer, classifier, retriever, generator, verifier, trace_logger\n",
    "    )\n",
    "    \n",
    "    console.print(\"[bold green]âœ“ Sistema inicializado exitosamente[/bold green]\\n\")\n",
    "    \n",
    "    # Loop interactivo\n",
    "    while True:\n",
    "        console.print(\"\\n\" + \"=\"*80)\n",
    "        query = console.input(\"[bold cyan]ğŸ‘¤ Ingresa tu consulta (o 'salir' para terminar): [/bold cyan]\")\n",
    "        \n",
    "        if query.lower() in ['salir', 'exit', 'quit']:\n",
    "            console.print(\"[bold green]Â¡Hasta luego![/bold green]\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        # Procesar consulta\n",
    "        result = orchestrator.process_query(query)\n",
    "        \n",
    "        # Mostrar respuesta\n",
    "        console.print(\"\\n\")\n",
    "        console.print(Panel(\n",
    "            f\"[bold white]{result['response']}[/bold white]\",\n",
    "            title=\"ğŸ¤– Respuesta del Sistema\",\n",
    "            border_style=\"green\"\n",
    "        ))\n",
    "        \n",
    "        # Mostrar informaciÃ³n adicional\n",
    "        if result.get(\"documents_used\"):\n",
    "            console.print(f\"\\n[cyan]ğŸ“š Documentos utilizados: {len(result['documents_used'])}[/cyan]\")\n",
    "        \n",
    "        console.print(f\"[cyan]ğŸ¯ IntenciÃ³n detectada: {result['intent']}[/cyan]\")\n",
    "        \n",
    "        if result.get(\"verification\"):\n",
    "            ver = result[\"verification\"]\n",
    "            console.print(f\"[cyan]âœ… Confianza de verificaciÃ³n: {ver.get('confianza', 0):.2f}[/cyan]\")\n",
    "        \n",
    "        # Mostrar trazabilidad\n",
    "        if console.input(\"\\n[yellow]Â¿Mostrar trazabilidad completa? (s/n): [/yellow]\").lower() == 's':\n",
    "            trace_logger.print_trace()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
